{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6118f849-f67c-45c5-a248-c7344569dbb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Modelado de Datos: Capa Gold en Azure Databricks\n",
    "\n",
    "Este notebook toma los datos ya transformados de la **capa Silver** y los organiza en un modelo de datos consumible para análisis, usualmente siguiendo un enfoque de modelo estrella o constelación (star schema), y escribe los resultados en la **capa Gold** utilizando Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa72179-71f0-4cfc-80f9-f7e797224a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5432cddc-ff8d-485f-9418-e36a8f50bfbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pyspark.sql.functions import col, current_timestamp, expr, lit\n",
    "from functools import reduce\n",
    "from operator import and_\n",
    "\n",
    "def leer_desde_silver(\n",
    "    nombre_tabla: str,\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    db_silver: str = \"silver_ventas\",\n",
    "    mode: str = \"full\",                 # \"full\" | \"diff\"\n",
    "    ts_col: str = \"fecha_carga\",\n",
    "    last_n_days: Optional[int] = None,  # ej. 2  -> últimos 2 días\n",
    "    last_n_hours: Optional[int] = None, # ej. 6  -> últimas 6 horas\n",
    "    since: Optional[str] = None,        # \"YYYY-MM-DD\" o \"YYYY-MM-DD HH:mm:ss\"\n",
    "    until: Optional[str] = None,        # límite superior EXCLUSIVO\n",
    "    drop_nulls: bool = True\n",
    "):\n",
    "    full = f\"{catalog_name}.{db_silver}.{nombre_tabla}\"\n",
    "    if not spark.catalog.tableExists(full):\n",
    "        raise ValueError(f\"La tabla {full} no existe en el metastore.\")\n",
    "\n",
    "    df = spark.table(full)\n",
    "\n",
    "    # Validar tipo timestamp\n",
    "    dtype = dict(df.dtypes).get(ts_col, \"\").lower()\n",
    "    if dtype != \"timestamp\":\n",
    "        raise ValueError(f\"La columna '{ts_col}' debe ser timestamp. Actual: {dtype}\")\n",
    "\n",
    "    if mode.lower() == \"full\":\n",
    "        return df if not drop_nulls else df.filter(col(ts_col).isNotNull())\n",
    "\n",
    "    # --- Diferencial por timestamp ---\n",
    "    conds = []\n",
    "    if drop_nulls:\n",
    "        conds.append(col(ts_col).isNotNull())\n",
    "\n",
    "    if last_n_days is not None:\n",
    "        conds.append(col(ts_col) >= (current_timestamp() - expr(f\"INTERVAL {int(last_n_days)} DAYS\")))\n",
    "    if last_n_hours is not None:\n",
    "        conds.append(col(ts_col) >= (current_timestamp() - expr(f\"INTERVAL {int(last_n_hours)} HOURS\")))\n",
    "    if since:\n",
    "        conds.append(col(ts_col) >= lit(since).cast(\"timestamp\"))\n",
    "    if until:\n",
    "        conds.append(col(ts_col) <  lit(until).cast(\"timestamp\"))  # upper bound abierto\n",
    "\n",
    "    if not conds:\n",
    "        raise ValueError(\"En mode='diff' especifica last_n_days/last_n_hours o since/until.\")\n",
    "\n",
    "    return df.filter(reduce(and_, conds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f0629e-781f-4e64-8c54-594c2f258543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Full\n",
    "#df_full = leer_desde_silver(\"t_camas\", db_silver=\"clinica_silver\", mode=\"full\")\n",
    "\n",
    "# Diferencial: últimos 2 días por 'fecalta'\n",
    "#df_diff_2d = leer_desde_silver(\"t_camas\", \"clinica_silver\", mode=\"diff\", date_col=\"fecalta\", last_n_days=2)\n",
    "\n",
    "# Diferencial: rango específico [2025-08-20, 2025-08-23)\n",
    "#df_diff_range = leer_desde_silver(\"t_camas\", \"clinica_silver\", mode=\"diff\",\n",
    " #                                 date_col=\"fecalta\", start=\"2025-08-20\", end=\"2025-08-23\")\n",
    "\n",
    "# Si 'fecalta' es string tipo \"YYYY-MM-DD HH:mm:ss\", ajusta el formato si difiere:\n",
    "# string_datetime_fmt=\"dd/MM/yyyy HH:mm:ss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508545ae-65bd-41aa-9a9d-374c9b7beb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "# Leer desde silver\n",
    "#df = leer_desde_silver(\"md_camas\", \"clinica_silver\", mode=\"diff\",\n",
    "#                           since=\"2025-08-20 00:00:00\", until=\"2025-08-23 00:00:00\")\n",
    "subcategoria = leer_desde_silver(\"md_subcategoria\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "categoria = leer_desde_silver(\"md_categoria\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "producto = leer_desde_silver(\"md_producto\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "ubigeo = leer_desde_silver(\"md_ubigeo\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "segmento = leer_desde_silver(\"md_segmento\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "mercado = leer_desde_silver(\"md_mercado\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "sectoreconomico = leer_desde_silver(\"md_sectoreconomico\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "# cliente = leer_desde_silver(\"md_cliente\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "vendedor = leer_desde_silver(\"md_vendedor\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "modalidadenvio = leer_desde_silver(\"md_modalidadenvio\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "modalidadventa = leer_desde_silver(\"md_modalidadventa\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "moneda = leer_desde_silver(\"md_moneda\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "mediopago = leer_desde_silver(\"md_mediopago\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "prioridadpedido = leer_desde_silver(\"md_prioridadpedido\", db_silver=\"silver_ventas\", mode=\"full\")\n",
    "pedido = leer_desde_silver(\"hd_pedido\", db_silver=\"silver_ventas\", mode=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eddb06fa-74de-472e-b0f0-52a4510c1535",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756098816014}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Realizando los JOINs de manera estructurada con ALIAS para mayor claridad\n",
    "df_Dimproducto = (\n",
    "    producto.alias(\"p\")\n",
    "    .join(subcategoria.alias(\"s\"), col(\"p.id_subcategoria\") == col(\"s.id_subcategoria\"), \"left\")\n",
    "    .join(categoria.alias(\"c\"), col(\"s.id_categoria\") == col(\"c.id_categoria\"), \"left\")\n",
    "    .drop(col(\"s.id_subcategoria\"), col(\"s.fecha_carga\"), col(\"s.year\"), col(\"s.month\"), col(\"s.day\"))\n",
    "    .drop(col(\"c.id_categoria\"), col(\"c.fecha_carga\"), col(\"c.year\"), col(\"c.month\"), col(\"c.day\"))\n",
    "    .drop(col(\"c._etl_source\"), col(\"c._etl_batch_id\"), col(\"c._etl_loaded_at\"))\n",
    "    .drop(col(\"s._etl_source\"), col(\"s._etl_batch_id\"), col(\"s._etl_loaded_at\"))\n",
    ")\n",
    "\n",
    "# Mostrar el esquema del DataFrame resultante\n",
    "df_Dimproducto.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Dimproducto.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09928050-a4fb-4329-9502-5a9537d1da30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Realizando los JOINs de manera estructurada con ALIAS para mayor claridad\n",
    "# df_DimCliente = (\n",
    "#     cliente.alias(\"cl\")\n",
    "#     .join(ubigeo.alias(\"ub\"), col(\"cl.id_ubigeo\") == col(\"ub.id_ubigeo\"), \"left\")\n",
    "#     .join(sectoreconomico.alias(\"se\"), col(\"cl.id_sectoreconomico\") == col(\"se.id_sectoreconomico\"), \"left\")\n",
    "#     .join(mercado.alias(\"me\"), col(\"cl.id_mercado\") == col(\"me.id_mercado\"), \"left\")\n",
    "#     .join(segmento.alias(\"sg\"), col(\"se.id_segmento\") == col(\"sg.id_segmento\"), \"left\")\n",
    "#     .drop(col(\"se.fecha_carga\"),col(\"ub.fecha_carga\"),col(\"me.fecha_carga\"),col(\"sg.fecha_carga\"), col(\"ub.id_ubigeo\"))\n",
    "#     .drop(col(\"me.id_mercado\"),col(\"sg.id_segmento\"),col(\"me.fecha_carga\"),col(\"se.id_sectoreconomico\"))\n",
    "#     .drop(col(\"ub.year\"), col(\"ub.month\"), col(\"ub.day\"), col(\"se.year\"), col(\"se.month\"), col(\"se.day\"))\n",
    "#     .drop(col(\"me.year\"), col(\"me.month\"), col(\"me.day\"), col(\"sg.year\"), col(\"sg.month\"), col(\"sg.day\"))\n",
    "#     .drop(col(\"ub._etl_source\"), col(\"ub._etl_batch_id\"), col(\"ub._etl_loaded_at\"))\n",
    "#     .drop(col(\"se._etl_source\"), col(\"se._etl_batch_id\"), col(\"se._etl_loaded_at\"))\n",
    "#     .drop(col(\"me._etl_source\"), col(\"me._etl_batch_id\"), col(\"me._etl_loaded_at\"))\n",
    "#     .drop(col(\"sg._etl_source\"), col(\"sg._etl_batch_id\"), col(\"sg._etl_loaded_at\"))\n",
    "# )\n",
    "\n",
    "# ## from cliente cl\n",
    "# #left join ubigeo ub on cl.CODUBIGEO = ub.CODUBIGEO\n",
    "# #left join sectoreconomico se on cl.CODSECTECON = se.CODSECTECON\n",
    "# #left join mercado me on cl.CODMRCADO = me.CODMRCADO\n",
    "# #left join segmento sg on se.CODSGMNTO = sg.CODSGMNTO\n",
    "\n",
    "# # Mostrar el esquema del DataFrame resultante\n",
    "# df_DimCliente.printSchema()\n",
    "\n",
    "# # Mostrar las primeras filas del DataFrame para ver la relación\n",
    "# df_DimCliente.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c945e555-bca0-463f-82c4-1ea424b9ff4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pedido.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1841da1f-1940-4437-a04c-fd42d66b964f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def crear_dim_tiempo(\n",
    "    df: DataFrame,\n",
    "    date_col: str,\n",
    "    *,\n",
    "    start_date: Optional[str] = None,   # \"YYYY-MM-DD\" opcional\n",
    "    end_date: Optional[str] = None,     # \"YYYY-MM-DD\" opcional\n",
    "    date_fmt: Optional[str] = None,     # ej. \"yyyy-MM-dd\" si es string\n",
    "    include_extras: bool = False        # agrega columnas adicionales\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Crea dimensión tiempo a partir de un DataFrame y una columna de fecha.\n",
    "    - date_col: columna en df (date/timestamp/string).\n",
    "    - start_date/end_date: sobrescriben los valores por defecto (min y max de la columna).\n",
    "    - date_fmt: formato si la columna es string (ej. \"yyyy/MM/dd\").\n",
    "    - include_extras: agrega columnas útiles como semana ISO, nombre de mes, etc.\n",
    "\n",
    "    Devuelve:\n",
    "      DataFrame con columnas:\n",
    "        id_tiempo (yyyymmdd int), fecha, anio, mes, dia, trimestre\n",
    "      + extras si include_extras=True\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Normaliza a date\n",
    "    if date_fmt:\n",
    "        fecha_expr = F.to_date(F.col(date_col), date_fmt)\n",
    "    else:\n",
    "        fecha_expr = F.to_date(F.col(date_col))\n",
    "\n",
    "    df_dates = df.select(fecha_expr.alias(\"_fecha_base\")).where(F.col(\"_fecha_base\").isNotNull())\n",
    "\n",
    "    # --- Determina rango de fechas (min y max del DF si no se pasan argumentos)\n",
    "    minmax = df_dates.agg(\n",
    "        F.min(\"_fecha_base\").alias(\"min_fecha\"),\n",
    "        F.max(\"_fecha_base\").alias(\"max_fecha\")\n",
    "    ).first()\n",
    "\n",
    "    if not minmax or minmax[\"min_fecha\"] is None or minmax[\"max_fecha\"] is None:\n",
    "        raise ValueError(f\"No se encontraron fechas válidas en la columna '{date_col}'.\")\n",
    "\n",
    "    start_date = start_date or minmax[\"min_fecha\"].strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date or minmax[\"max_fecha\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # --- Genera secuencia de fechas\n",
    "    df_rango = (\n",
    "        df.sparkSession.createDataFrame([(1,)], [\"dummy\"])\n",
    "        .select(F.sequence(\n",
    "            F.to_date(F.lit(start_date)),\n",
    "            F.to_date(F.lit(end_date)),\n",
    "            F.expr(\"interval 1 day\")\n",
    "        ).alias(\"fechas\"))\n",
    "    )\n",
    "    df_fechas = df_rango.select(F.explode(F.col(\"fechas\")).alias(\"fecha\"))\n",
    "\n",
    "    # --- Atributos base\n",
    "    df_dim = (\n",
    "        df_fechas\n",
    "        .select(\n",
    "            F.col(\"fecha\").cast(\"date\").alias(\"fecha\"),\n",
    "            F.year(\"fecha\").cast(\"int\").alias(\"anio\"),\n",
    "            F.month(\"fecha\").cast(\"int\").alias(\"mes\"),\n",
    "            F.dayofmonth(\"fecha\").cast(\"int\").alias(\"dia\"),\n",
    "            F.quarter(\"fecha\").cast(\"int\").alias(\"trimestre\")\n",
    "        )\n",
    "        .withColumn(\"id_tiempo\",\n",
    "            (F.col(\"anio\") * F.lit(10000) + F.col(\"mes\") * F.lit(100) + F.col(\"dia\")).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Extras opcionales\n",
    "    if include_extras:\n",
    "        df_dim = (\n",
    "            df_dim\n",
    "            .withColumn(\"anio_mes\", (F.col(\"anio\")*100 + F.col(\"mes\")).cast(\"int\"))\n",
    "            .withColumn(\"semana_iso\", F.weekofyear(\"fecha\").cast(\"int\"))\n",
    "            .withColumn(\"dia_semana_iso\", F.date_format(\"fecha\", \"u\").cast(\"int\"))  # 1=lunes..7=domingo\n",
    "            .withColumn(\"nombre_mes\", F.date_format(\"fecha\", \"MMMM\"))\n",
    "            .withColumn(\"nombre_dia\", F.date_format(\"fecha\", \"EEEE\"))\n",
    "            .withColumn(\"es_fin_de_semana\", F.when(F.col(\"dia_semana_iso\") >= 6, 1).otherwise(0))\n",
    "            .withColumn(\"primer_dia_mes\", F.trunc(\"fecha\", \"month\"))\n",
    "            .withColumn(\"ultimo_dia_mes\", F.last_day(\"fecha\"))\n",
    "            .withColumn(\"primer_dia_trimestre\",\n",
    "                        F.expr(\"make_date(anio, ((trimestre-1)*3)+1, 1)\"))\n",
    "            .withColumn(\"primer_dia_anio\", F.trunc(\"fecha\", \"year\"))\n",
    "        )\n",
    "\n",
    "    # --- Orden de columnas\n",
    "    base_cols = [\"id_tiempo\", \"fecha\", \"anio\", \"mes\", \"dia\", \"trimestre\"]\n",
    "    extra_cols = [\n",
    "        \"anio_mes\", \"semana_iso\", \"dia_semana_iso\", \"nombre_mes\", \"nombre_dia\",\n",
    "        \"es_fin_de_semana\", \"primer_dia_mes\", \"ultimo_dia_mes\",\n",
    "        \"primer_dia_trimestre\", \"primer_dia_anio\"\n",
    "    ]\n",
    "    ordered = base_cols + [c for c in extra_cols if c in df_dim.columns]\n",
    "\n",
    "    return df_dim.select(*ordered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7af1ebb-98ca-4c1d-ac53-5eb568016d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Si tu DF tiene columna \"fec_alta\"\n",
    "df_dimtiempo = crear_dim_tiempo(pedido,\"FECPEDID\")\n",
    "\n",
    "# Forzar rango manualmente\n",
    "df_dimtiempo = crear_dim_tiempo(df_clientes, \"fec_alta\",\n",
    "                            start_date=\"2020-01-01\", end_date=\"2025-12-31\",\n",
    "                             include_extras=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3123f7c-9310-4575-888a-617f34f39493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dimtiempo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b839126-c3b1-45a0-a2b1-ae3589239622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG desarrollo;\n",
    "\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0257434f-2a08-4ea1-bbd6-c88a2113904f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def crear_tabla_delta_merge_managed(\n",
    "    nombre_df: str,\n",
    "    nombre_tabla: str,\n",
    "    llave_origen: List[str],\n",
    "    llave_destino: List[str],\n",
    "    db_name: str = \"default\",\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    auto_merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea si no existe una tabla Delta GESTIONADA en la base (que ya debe tener LOCATION en tu mount)\n",
    "    y realiza MERGE. No usa LOCATION explícito.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validaciones\n",
    "    df = globals()[nombre_df]\n",
    "\n",
    "    if len(llave_origen) != len(llave_destino):\n",
    "        print(\"❌ Error: La cantidad de columnas en 'llave_origen' y 'llave_destino' no coinciden.\")\n",
    "        return\n",
    "\n",
    "    if partition_cols:\n",
    "        faltantes = [c for c in partition_cols if c not in df.columns]\n",
    "        if faltantes:\n",
    "            print(f\"❌ Error: Columnas de partición no existen en el DataFrame: {faltantes}\")\n",
    "            return\n",
    "\n",
    "    if auto_merge_schema:\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Armar nombre completo\n",
    "    full_name = f\"{catalog_name}.{db_name}.{nombre_tabla}\"\n",
    "\n",
    "    # ✅ FIX: usar el overload moderno (una sola cadena)\n",
    "    exists = spark.catalog.tableExists(full_name)\n",
    "\n",
    "    if not exists:\n",
    "        # Crear como TABLA GESTIONADA en el LOCATION de la DB (sin LOCATION explícito)\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_cols:\n",
    "            # ✅ FIX: varargs\n",
    "            writer = writer.partitionBy(*partition_cols)\n",
    "        writer.saveAsTable(full_name)\n",
    "        print(f\"✅ Tabla gestionada creada: {full_name} (bajo LOCATION de la base '{db_name}')\")\n",
    "        return\n",
    "\n",
    "    # Si existe, MERGE\n",
    "    try:\n",
    "        delta_tbl = DeltaTable.forName(spark, full_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"❌ La tabla {full_name} no es Delta o no es accesible como Delta: {e}\")\n",
    "\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"tgt.`{llave_destino[i]}` = src.`{llave_origen[i]}`\" for i in range(len(llave_origen))]\n",
    "    )\n",
    "    set_expr  = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "    vals_expr = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "\n",
    "    print(f\"🔄 Ejecutando MERGE INTO {full_name} ...\")\n",
    "    (delta_tbl.alias(\"tgt\")\n",
    "             .merge(df.alias(\"src\"), merge_condition)\n",
    "             .whenMatchedUpdate(set=set_expr)\n",
    "             .whenNotMatchedInsert(values=vals_expr)\n",
    "             .execute())\n",
    "    print(f\"✅ MERGE completado para {full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0da091-8414-4f17-83a1-9ed0124ac399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar la función para crear la tabla y hacer MERGE usando diferentes llaves\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"moneda\",\n",
    "    nombre_tabla=\"dim_moneda\",\n",
    "    llave_origen=[\"id_moneda\"],\n",
    "    llave_destino=[\"id_moneda\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f82b98-4ee7-40cd-9b10-ca48467c61ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Dimproducto\",\n",
    "    nombre_tabla=\"dim_producto\",\n",
    "    llave_origen=[\"id_producto\"],\n",
    "    llave_destino=[\"id_producto\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1abc18-3a9f-458c-a088-d57cc84f5da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# crear_tabla_delta_merge_managed(\n",
    "#     nombre_df=\"df_DimCliente\",\n",
    "#     nombre_tabla=\"dim_cliente\",\n",
    "#     llave_origen=[\"id_cliente\"],\n",
    "#     llave_destino=[\"id_cliente\"],\n",
    "#     db_name=\"gold_ventas\",\n",
    "#     partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5130b942-2344-4205-b1fc-7da935aeb915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"vendedor\",\n",
    "    nombre_tabla=\"dim_vendedor\",\n",
    "    llave_origen=[\"id_vendedor\"],\n",
    "    llave_destino=[\"id_vendedor\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8120bf-6d52-4e11-90b2-88d375b4313d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"mediopago\",\n",
    "    nombre_tabla=\"dim_mediopago\",\n",
    "    llave_origen=[\"id_mediopago\"],\n",
    "    llave_destino=[\"id_mediopago\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2406b4c0-fe57-4094-9277-1b3e42a28bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"prioridadpedido\",\n",
    "    nombre_tabla=\"dim_prioridadpedido\",\n",
    "    llave_origen=[\"id_prioridadpedido\"],\n",
    "    llave_destino=[\"id_prioridadpedido\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cdbe236-2758-4447-b622-9c927b7e2552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"modalidadenvio\",\n",
    "    nombre_tabla=\"dim_modalidadenvio\",\n",
    "    llave_origen=[\"id_modalidadenvio\"],\n",
    "    llave_destino=[\"id_modalidadenvio\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dc61a1-0822-40ac-a95c-8520741e3241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"modalidadventa\",\n",
    "    nombre_tabla=\"dim_modalidadventa\",\n",
    "    llave_origen=[\"id_modalidadventa\"],\n",
    "    llave_destino=[\"id_modalidadventa\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc74b12-a003-4228-b4bc-b6ab5055c874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"pedido\",\n",
    "    nombre_tabla=\"fact_pedido\",\n",
    "    llave_origen=[\"id_pedido\"],\n",
    "    llave_destino=[\"id_pedido\"],\n",
    "    db_name=\"gold_ventas\",\n",
    "    partition_cols=[\"fecha_carga\"]  # opcional; si no quieres partición, quítalo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39833f58-492e-4eb2-a80b-37db161bc8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dimtiempo:pyspark.sql.connect.dataframe.DataFrame\n",
    "id_tiempo:integer\n",
    "fecha:date\n",
    "anio:integer\n",
    "mes:integer\n",
    "dia:integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7929306e-ae13-43f4-9259-bb7b1a57da96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_dimtiempo\",\n",
    "    nombre_tabla=\"dim_tiempo\",\n",
    "    llave_origen=[\"df_dimtiempo\"],\n",
    "    llave_destino=[\"df_dimtiempo\"],\n",
    "    db_name=\"gold_ventas\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3522282755143255,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc_ventas03_modelado_gold_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
