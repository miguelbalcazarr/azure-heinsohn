{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6118f849-f67c-45c5-a248-c7344569dbb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Modelado de Datos: Capa Gold en Azure Databricks\n",
    "\n",
    "Este notebook toma los datos ya transformados de la **capa Silver** y los organiza en un modelo de datos consumible para an√°lisis, usualmente siguiendo un enfoque de modelo estrella o constelaci√≥n (star schema), y escribe los resultados en la **capa Gold** utilizando Delta Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa72179-71f0-4cfc-80f9-f7e797224a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5432cddc-ff8d-485f-9418-e36a8f50bfbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pyspark.sql.functions import col, current_timestamp, expr, lit\n",
    "from functools import reduce\n",
    "from operator import and_\n",
    "\n",
    "def leer_desde_silver(\n",
    "    nombre_tabla: str,\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    db_silver: str = \"silver_clinica\",\n",
    "    mode: str = \"full\",                 # \"full\" | \"diff\"\n",
    "    ts_col: str = \"fecalta\",\n",
    "    last_n_days: Optional[int] = None,  # ej. 2  -> √∫ltimos 2 d√≠as\n",
    "    last_n_hours: Optional[int] = None, # ej. 6  -> √∫ltimas 6 horas\n",
    "    since: Optional[str] = None,        # \"YYYY-MM-DD\" o \"YYYY-MM-DD HH:mm:ss\"\n",
    "    until: Optional[str] = None,        # l√≠mite superior EXCLUSIVO\n",
    "    drop_nulls: bool = True\n",
    "):\n",
    "    full = f\"{catalog_name}.{db_silver}.{nombre_tabla}\"\n",
    "    if not spark.catalog.tableExists(full):\n",
    "        raise ValueError(f\"La tabla {full} no existe en el metastore.\")\n",
    "\n",
    "    df = spark.table(full)\n",
    "\n",
    "    # Validar tipo timestamp\n",
    "    dtype = dict(df.dtypes).get(ts_col, \"\").lower()\n",
    "    if dtype != \"timestamp\":\n",
    "        raise ValueError(f\"La columna '{ts_col}' debe ser timestamp. Actual: {dtype}\")\n",
    "\n",
    "    if mode.lower() == \"full\":\n",
    "        return df if not drop_nulls else df.filter(col(ts_col).isNotNull())\n",
    "\n",
    "    # --- Diferencial por timestamp ---\n",
    "    conds = []\n",
    "    if drop_nulls:\n",
    "        conds.append(col(ts_col).isNotNull())\n",
    "\n",
    "    if last_n_days is not None:\n",
    "        conds.append(col(ts_col) >= (current_timestamp() - expr(f\"INTERVAL {int(last_n_days)} DAYS\")))\n",
    "    if last_n_hours is not None:\n",
    "        conds.append(col(ts_col) >= (current_timestamp() - expr(f\"INTERVAL {int(last_n_hours)} HOURS\")))\n",
    "    if since:\n",
    "        conds.append(col(ts_col) >= lit(since).cast(\"timestamp\"))\n",
    "    if until:\n",
    "        conds.append(col(ts_col) <  lit(until).cast(\"timestamp\"))  # upper bound abierto\n",
    "\n",
    "    if not conds:\n",
    "        raise ValueError(\"En mode='diff' especifica last_n_days/last_n_hours o since/until.\")\n",
    "\n",
    "    return df.filter(reduce(and_, conds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f0629e-781f-4e64-8c54-594c2f258543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Full\n",
    "#df_full = leer_desde_silver(\"t_camas\", db_silver=\"clinica_silver\", mode=\"full\")\n",
    "\n",
    "# Diferencial: √∫ltimos 2 d√≠as por 'fecalta'\n",
    "#df_diff_2d = leer_desde_silver(\"t_camas\", \"clinica_silver\", mode=\"diff\", date_col=\"fecalta\", last_n_days=2)\n",
    "\n",
    "# Diferencial: rango espec√≠fico [2025-08-20, 2025-08-23)\n",
    "#df_diff_range = leer_desde_silver(\"t_camas\", \"clinica_silver\", mode=\"diff\",\n",
    " #                                 date_col=\"fecalta\", start=\"2025-08-20\", end=\"2025-08-23\")\n",
    "\n",
    "# Si 'fecalta' es string tipo \"YYYY-MM-DD HH:mm:ss\", ajusta el formato si difiere:\n",
    "# string_datetime_fmt=\"dd/MM/yyyy HH:mm:ss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508545ae-65bd-41aa-9a9d-374c9b7beb08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "# Leer desde silver\n",
    "#df_Camas = leer_desde_silver(\"md_camas\", \"clinica_silver\", mode=\"diff\",\n",
    "#                           since=\"2025-08-20 00:00:00\", until=\"2025-08-23 00:00:00\")\n",
    "df_Camas = leer_desde_silver(\"md_camas\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Camas.printSchema()\n",
    "\n",
    "# Contar filas \n",
    "print(f\"Total de filas: {df_Camas.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5faaaf4b-4000-4894-883f-3e6e91809867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_Habitaciones = leer_desde_silver(\"md_habitaciones\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Habitaciones.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Habitaciones.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c45667-12c1-47f5-81fb-07ea932958c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde silver\n",
    "df_Medicos = leer_desde_silver(\"md_medicos\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Medicos.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Medicos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0eabb7d-f46b-4707-a063-01adf8493b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde silver\n",
    "df_Internamientos = leer_desde_silver(\"md_internamientos\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Internamientos.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Internamientos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bff6ee-8dba-4dc2-9473-3c5246740be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde silver\n",
    "df_Pacientes = leer_desde_silver(\"md_pacientes\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Pacientes.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Pacientes.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272646ca-3777-4ab8-8fa7-ac562218c58d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde silver\n",
    "df_VisitasMedicas = leer_desde_silver(\"md_visitas_medicas\", db_silver=\"silver_clinica\", mode=\"full\")\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_VisitasMedicas.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_VisitasMedicas.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1fce87-726c-4db9-b112-31be880f1292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizamos left join: habitaciones ‚Üê camas\n",
    "df_HabitacionCama = df_Habitaciones.join(\n",
    "    df_Camas,\n",
    "    df_Habitaciones[\"numero_habitacion\"] == df_Camas[\"numero_habitacion\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# (Opcional) eliminamos columna duplicada si existe\n",
    "df_HabitacionCama = df_HabitacionCama.drop(df_Camas[\"numero_habitacion\"],df_Camas[\"year\"], df_Camas[\"month\"], df_Camas[\"day\"], df_Camas[\"fecalta\"], df_Camas[\"usralta\"], df_Camas[\"estado\"])\n",
    "\n",
    "# Mostrar resultado\n",
    "df_HabitacionCama.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91f6de3-de5e-4ff9-9019-6ce6cb1c9ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min as min_, sequence, explode, to_date, year, month, dayofmonth, quarter, lit\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "\n",
    "def crear_dim_tiempo(df_par):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame de dimensi√≥n tiempo desde la primera fecha de 'fecalta' hasta hoy.\n",
    "\n",
    "    Par√°metros:\n",
    "    - df_internamientos: DataFrame con columna 'fecalta'\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame con columnas: fecha, a√±o, mes, d√≠a, trimestre\n",
    "    \"\"\"\n",
    "    # Obtener fecha m√≠nima de alta desde el DataFrame\n",
    "    fecha_min = df_par.select(min_(\"fecalta\")).first()[0]\n",
    "    fecha_min_str = fecha_min.strftime(\"%Y-%m-%d\") if isinstance(fecha_min, datetime) else fecha_min\n",
    "\n",
    "    # Crear DataFrame con rango de fechas desde fecha m√≠nima hasta hoy\n",
    "    df_rango = spark.sql(f\"SELECT sequence(to_date('{fecha_min_str}'), current_date(), interval 1 day) AS fechas\")\n",
    "    df_fechas = df_rango.select(explode(col(\"fechas\")).alias(\"fecha\"))\n",
    "\n",
    "    # Agregar columnas de atributos de tiempo\n",
    "    df_dim_tiempo = df_fechas.select(\n",
    "        col(\"fecha\").alias(\"fecha\"),\n",
    "        year(\"fecha\").alias(\"anio\"),\n",
    "        month(\"fecha\").alias(\"mes\"),\n",
    "        dayofmonth(\"fecha\").alias(\"dia\"),\n",
    "        quarter(\"fecha\").alias(\"trimestre\")\n",
    "    )\n",
    "\n",
    "    return df_dim_tiempo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9f0c05-dfd0-49f7-9e24-b4f5c79c0cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dim_tiempo = crear_dim_tiempo(df_Internamientos)\n",
    "df_dim_tiempo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b839126-c3b1-45a0-a2b1-ae3589239622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG desarrollo;\n",
    "\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0257434f-2a08-4ea1-bbd6-c88a2113904f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def crear_tabla_delta_merge_managed(\n",
    "    nombre_df: str,\n",
    "    nombre_tabla: str,\n",
    "    llave_origen: List[str],\n",
    "    llave_destino: List[str],\n",
    "    db_name: str = \"default\",\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    auto_merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea si no existe una tabla Delta GESTIONADA en la base (que ya debe tener LOCATION en tu mount)\n",
    "    y realiza MERGE. No usa LOCATION expl√≠cito.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validaciones\n",
    "    df = globals()[nombre_df]\n",
    "\n",
    "    if len(llave_origen) != len(llave_destino):\n",
    "        print(\"‚ùå Error: La cantidad de columnas en 'llave_origen' y 'llave_destino' no coinciden.\")\n",
    "        return\n",
    "\n",
    "    if partition_cols:\n",
    "        faltantes = [c for c in partition_cols if c not in df.columns]\n",
    "        if faltantes:\n",
    "            print(f\"‚ùå Error: Columnas de partici√≥n no existen en el DataFrame: {faltantes}\")\n",
    "            return\n",
    "\n",
    "    if auto_merge_schema:\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Armar nombre completo\n",
    "    full_name = f\"{catalog_name}.{db_name}.{nombre_tabla}\"\n",
    "\n",
    "    # ‚úÖ FIX: usar el overload moderno (una sola cadena)\n",
    "    exists = spark.catalog.tableExists(full_name)\n",
    "\n",
    "    if not exists:\n",
    "        # Crear como TABLA GESTIONADA en el LOCATION de la DB (sin LOCATION expl√≠cito)\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_cols:\n",
    "            # ‚úÖ FIX: varargs\n",
    "            writer = writer.partitionBy(*partition_cols)\n",
    "        writer.saveAsTable(full_name)\n",
    "        print(f\"‚úÖ Tabla gestionada creada: {full_name} (bajo LOCATION de la base '{db_name}')\")\n",
    "        return\n",
    "\n",
    "    # Si existe, MERGE\n",
    "    try:\n",
    "        delta_tbl = DeltaTable.forName(spark, full_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå La tabla {full_name} no es Delta o no es accesible como Delta: {e}\")\n",
    "\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"tgt.`{llave_destino[i]}` = src.`{llave_origen[i]}`\" for i in range(len(llave_origen))]\n",
    "    )\n",
    "    set_expr  = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "    vals_expr = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "\n",
    "    print(f\"üîÑ Ejecutando MERGE INTO {full_name} ...\")\n",
    "    (delta_tbl.alias(\"tgt\")\n",
    "             .merge(df.alias(\"src\"), merge_condition)\n",
    "             .whenMatchedUpdate(set=set_expr)\n",
    "             .whenNotMatchedInsert(values=vals_expr)\n",
    "             .execute())\n",
    "    print(f\"‚úÖ MERGE completado para {full_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb0da091-8414-4f17-83a1-9ed0124ac399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar la funci√≥n para crear la tabla y hacer MERGE usando diferentes llaves\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_HabitacionCama\",\n",
    "    nombre_tabla=\"dim_habitacion\",\n",
    "    llave_origen=[\"numero_cama\"],\n",
    "    llave_destino=[\"numero_cama\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Pacientes\",\n",
    "    nombre_tabla=\"dim_pacientes\",\n",
    "    llave_origen=[\"id_paciente\"],\n",
    "    llave_destino=[\"id_paciente\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Medicos\",\n",
    "    nombre_tabla=\"dim_medicos\",\n",
    "    llave_origen=[\"id_medico\"],\n",
    "    llave_destino=[\"id_medico\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Internamientos\",\n",
    "    nombre_tabla=\"fact_internamientos\",\n",
    "    llave_origen=[\"id_internamiento\"],\n",
    "    llave_destino=[\"id_internamiento\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_VisitasMedicas\",\n",
    "    nombre_tabla=\"fact_visitas_medicas\",\n",
    "    llave_origen=[\"id_visita\"],\n",
    "    llave_destino=[\"id_visita\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_dim_tiempo\",\n",
    "    nombre_tabla=\"dim_tiempo\",\n",
    "    llave_origen=[\"fecha\"],\n",
    "    llave_destino=[\"fecha\"],\n",
    "    db_name=\"gold_clinica\",\n",
    "    partition_cols=[\"fecha\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3522282755143152,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc_clinica03_modelado_gold_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
