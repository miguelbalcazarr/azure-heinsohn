{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5772ba27-ef3e-44a0-a2ce-64579c5cd544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# ExtracciÃ³n de Datos a la Capa Bronze en Azure Databricks\n",
    "\n",
    "Este notebook implementa el proceso de extracciÃ³n de datos desde las tablas operativas del sistema de gestiÃ³n de internamientos clÃ­nicos hacia la capa Bronze del Data Lakehouse en Azure, utilizando Delta Lake con tablas gestionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbac2c4-47dd-49a7-bba0-c35fce7acfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Consulta a las tablas del Origen de datos y traerla a Dataframe\n",
    "Definimos una funcion para conectarnos a SQL Server y cargar todo a un Dataframe Generico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be11ad0a-3b4c-4f86-ab7a-a6f0f30e4c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def leer_tablas_sql_azure(lista_tablas, lista_modos_carga, fecha_corte, url_conexion, usuario, password,\n",
    "                          esquemas=None, columna_incremental=\"fecalta\"):\n",
    "    \"\"\"\n",
    "    Lee mÃºltiples tablas desde Azure SQL Database, usando carga full o incremental por tabla.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "    - lista_tablas: lista de nombres de tablas a leer.\n",
    "    - lista_modos_carga: lista del mismo tamaÃ±o que lista_tablas con 'full' o 'incremental'.\n",
    "    - fecha_corte: fecha para filtro incremental.\n",
    "    - url_conexion: cadena JDBC a Azure SQL.\n",
    "    - usuario: nombre de usuario de conexiÃ³n.\n",
    "    - password: contraseÃ±a.\n",
    "    - esquemas: dict opcional {tabla: esquema}, por defecto usa 'dbo'.\n",
    "    - columna_incremental: nombre de la columna de fecha para carga incremental.\n",
    "\n",
    "    Retorna:\n",
    "    - None. Crea variables globales con los DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(lista_tablas) != len(lista_modos_carga):\n",
    "        raise ValueError(\"Las listas 'lista_tablas' y 'lista_modos_carga' deben tener la misma longitud.\")\n",
    "\n",
    "    properties = {\n",
    "        \"user\": usuario,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "\n",
    "    for i, nombre_tabla in enumerate(lista_tablas):\n",
    "        modo_carga = lista_modos_carga[i].lower()\n",
    "        esquema = esquemas.get(nombre_tabla, \"dbo\") if esquemas else \"dbo\"\n",
    "        tabla_completa = f\"{esquema}.{nombre_tabla}\"\n",
    "        query = \"\"\n",
    "        base_query = f\"(SELECT * FROM {tabla_completa}) AS temp\"\n",
    "\n",
    "        try:\n",
    "            # Validar existencia de columna incremental\n",
    "            df_sample = spark.read.jdbc(url=url_conexion, table=base_query, properties=properties).limit(10)\n",
    "\n",
    "            if modo_carga == \"incremental\":\n",
    "                if columna_incremental in df_sample.columns:\n",
    "                    query = f\"(SELECT * FROM {tabla_completa} WHERE {columna_incremental} = '{fecha_corte}') AS temp\"\n",
    "                else:\n",
    "                    print(f\"âš ï¸  Tabla '{tabla_completa}' sin columna '{columna_incremental}'. Se leerÃ¡ completa.\")\n",
    "                    query = base_query\n",
    "            else:\n",
    "                query = base_query\n",
    "\n",
    "            df = spark.read.jdbc(url=url_conexion, table=query, properties=properties)\n",
    "            globals()[nombre_tabla] = df\n",
    "\n",
    "            print(f\"âœ… DataFrame '{nombre_tabla}' ({modo_carga}) cargado desde '{tabla_completa}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error al leer la tabla '{tabla_completa}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d7a071-3f0c-438d-b922-6a9f17ea4183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ParÃ¡metros de conexiÃ³n\n",
    "url_conexion = dbutils.secrets.get(\"scope-dev\", \"secret-sql-url\")\n",
    "usuario = dbutils.secrets.get(\"scope-dev\", \"secret-sql-user\")\n",
    "password = dbutils.secrets.get(\"scope-dev\", \"secret-sql-password\")\n",
    "fecha_carga = \"2025-04-14\"\n",
    "#properties = {\"user\": \"admin01juls\", \"password\": \"287719Julius@12\", \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "\n",
    "# Lista de tablas a leer\n",
    "lista_tablas = [\"Camas\", \"Habitaciones\", \"Medicos\", \"Pacientes\",\"Internamientos\",\"Visitas_Medicas\"]\n",
    "esquemas = {\"Camas\": \"st_thomas\", \"Visitas_Medicas\": \"St_thomas\", \"Habitaciones\": \"st_thomas\", \"Medicos\": \"st_thomas\", \"Pacientes\": \"st_thomas\", \"Internamientos\": \"st_thomas\"}  # pacientes usa 'dbo' por defecto\n",
    "modos = [\"incremental\", \"full\", \"incremental\", \"incremental\", \"full\", \"full\"]\n",
    "\n",
    "# Llamar la funciÃ³n para crear DataFrames individuales\n",
    "leer_tablas_sql_azure(lista_tablas, modos, fecha_carga, url_conexion, usuario, password, esquemas, columna_incremental=\"fecalta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477c9e26-cf54-4760-b625-3a4e3719bc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, lit , to_timestamp, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "\n",
    "df_Camas= Camas.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Camas.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_Camas.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8daed60f-7f5c-4407-a1bc-b5a3b6e257ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Habitaciones = Habitaciones.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Habitaciones.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_Habitaciones.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85763ad2-6ea1-411c-aad2-46a1ccfc5be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Pacientes = Pacientes.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Pacientes.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_Pacientes.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2fde24-1ff0-40eb-8ba7-f2ec3a7db257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Medicos = Medicos.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Medicos.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_Medicos.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6931f02f-52df-45b0-beb9-3365e45f3409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Internamientos = Internamientos.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Internamientos.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_Internamientos.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb22a931-3ced-4287-85db-aeaecd50fe51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_VisitasMedicas = Visitas_Medicas.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_VisitasMedicas.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relaciÃ³n\n",
    "df_VisitasMedicas.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c20368-3c36-4f7b-bfaa-c7babef14f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1464b6-cad3-42fe-bc79-967651597945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def crear_tabla_delta_merge_managed(\n",
    "    nombre_df: str,\n",
    "    nombre_tabla: str,\n",
    "    llave_origen: List[str],\n",
    "    llave_destino: List[str],\n",
    "    db_name: str = \"default\",\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    auto_merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea si no existe una tabla Delta GESTIONADA en la base (que ya debe tener LOCATION en tu mount)\n",
    "    y realiza MERGE. No usa LOCATION explÃ­cito.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validaciones\n",
    "    df = globals()[nombre_df]\n",
    "\n",
    "    if len(llave_origen) != len(llave_destino):\n",
    "        print(\"âŒ Error: La cantidad de columnas en 'llave_origen' y 'llave_destino' no coinciden.\")\n",
    "        return\n",
    "\n",
    "    if partition_cols:\n",
    "        faltantes = [c for c in partition_cols if c not in df.columns]\n",
    "        if faltantes:\n",
    "            print(f\"âŒ Error: Columnas de particiÃ³n no existen en el DataFrame: {faltantes}\")\n",
    "            return\n",
    "\n",
    "    if auto_merge_schema:\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Armar nombre completo\n",
    "    full_name = f\"{catalog_name}.{db_name}.{nombre_tabla}\"\n",
    "\n",
    "    # âœ… FIX: usar el overload moderno (una sola cadena)\n",
    "    exists = spark.catalog.tableExists(full_name)\n",
    "\n",
    "    if not exists:\n",
    "        # Crear como TABLA GESTIONADA en el LOCATION de la DB (sin LOCATION explÃ­cito)\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_cols:\n",
    "            # âœ… FIX: varargs\n",
    "            writer = writer.partitionBy(*partition_cols)\n",
    "        writer.saveAsTable(full_name)\n",
    "        print(f\"âœ… Tabla gestionada creada: {full_name} (bajo LOCATION de la base '{db_name}')\")\n",
    "        return\n",
    "\n",
    "    # Si existe, MERGE\n",
    "    try:\n",
    "        delta_tbl = DeltaTable.forName(spark, full_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"âŒ La tabla {full_name} no es Delta o no es accesible como Delta: {e}\")\n",
    "\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"tgt.`{llave_destino[i]}` = src.`{llave_origen[i]}`\" for i in range(len(llave_origen))]\n",
    "    )\n",
    "    set_expr  = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "    vals_expr = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "\n",
    "    print(f\"ðŸ”„ Ejecutando MERGE INTO {full_name} ...\")\n",
    "    (delta_tbl.alias(\"tgt\")\n",
    "             .merge(df.alias(\"src\"), merge_condition)\n",
    "             .whenMatchedUpdate(set=set_expr)\n",
    "             .whenNotMatchedInsert(values=vals_expr)\n",
    "             .execute())\n",
    "    print(f\"âœ… MERGE completado para {full_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895b119a-35b6-4a24-b483-a6e53f20049e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar la funciÃ³n para crear la tabla y hacer MERGE usando diferentes llaves\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Camas\",\n",
    "    nombre_tabla=\"t_camas\",\n",
    "    llave_origen=[\"numero_cama\"],\n",
    "    llave_destino=[\"numero_cama\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Habitaciones\",\n",
    "    nombre_tabla=\"t_habitaciones\",\n",
    "    llave_origen=[\"numero_habitacion\"],\n",
    "    llave_destino=[\"numero_habitacion\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Pacientes\",\n",
    "    nombre_tabla=\"t_pacientes\",\n",
    "    llave_origen=[\"id_paciente\"],\n",
    "    llave_destino=[\"id_paciente\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Medicos\",\n",
    "    nombre_tabla=\"t_medicos\",\n",
    "    llave_origen=[\"id_medico\"],\n",
    "    llave_destino=[\"id_medico\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Internamientos\",\n",
    "    nombre_tabla=\"t_internamientos\",\n",
    "    llave_origen=[\"id_internamiento\"],\n",
    "    llave_destino=[\"id_internamiento\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_VisitasMedicas\",\n",
    "    nombre_tabla=\"t_visitas_medicas\",\n",
    "    llave_origen=[\"id_visita\"],\n",
    "    llave_destino=[\"id_visita\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres particiÃ³n, quÃ­talo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c70a35-f0c8-4d6a-b88f-bc3e0e3ded45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tune_delta_table(db_name: str, table_name: str, catalog_name: str = \"desarrollo\", zorder_cols=None, retention_hours: int = 168) -> None:\n",
    "    \"\"\"\n",
    "    Ajusta una tabla Delta con OPTIMIZE (+ZORDER opcional) y VACUUM.\n",
    "    - retention_hours >= 168 (7 dÃ­as) para cumplir la regla por defecto.\n",
    "    \"\"\"\n",
    "    full = f\"{catalog_name}.{db_name}.{table_name}\"\n",
    "\n",
    "    # Buenas prÃ¡cticas por sesiÃ³n (puedes mover a init script/cluster)\n",
    "    spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "    # Asegura propiedades en la tabla\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {full} SET TBLPROPERTIES (\n",
    "          delta.autoOptimize.optimizeWrite = true,\n",
    "          delta.autoOptimize.autoCompact = true\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # OPTIMIZE (+ ZORDER opcional)\n",
    "    if zorder_cols:\n",
    "        cols = \", \".join([f\"`{c}`\" for c in zorder_cols])\n",
    "        spark.sql(f\"OPTIMIZE {full} ZORDER BY ({cols})\")\n",
    "    else:\n",
    "        spark.sql(f\"OPTIMIZE {full}\")\n",
    "\n",
    "    # VACUUM con retenciÃ³n segura (>= 7 dÃ­as)\n",
    "    spark.sql(f\"VACUUM {full} RETAIN {retention_hours} HOURS\")\n",
    "    print(f\"âœ… OPTIMIZE/VACUUM completados para {full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f0ec8f-d6cf-49bb-982d-209a1c6869e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tune_delta_table(\"bronze_clinica\", \"t_camas\", zorder_cols=[\"numero_cama\"])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3555005416841897,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc_clinica01_extraccion_bronze_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
