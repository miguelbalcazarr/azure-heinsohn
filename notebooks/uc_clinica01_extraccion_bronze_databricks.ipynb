{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5772ba27-ef3e-44a0-a2ce-64579c5cd544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Extracción de Datos a la Capa Bronze en Azure Databricks\n",
    "\n",
    "Este notebook implementa el proceso de extracción de datos desde las tablas operativas del sistema de gestión de internamientos clínicos hacia la capa Bronze del Data Lakehouse en Azure, utilizando Delta Lake con tablas gestionadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbac2c4-47dd-49a7-bba0-c35fce7acfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Consulta a las tablas del Origen de datos y traerla a Dataframe\n",
    "Definimos una funcion para conectarnos a SQL Server y cargar todo a un Dataframe Generico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be11ad0a-3b4c-4f86-ab7a-a6f0f30e4c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def leer_tablas_sql_azure(lista_tablas, lista_modos_carga, fecha_corte, url_conexion, usuario, password,\n",
    "                          esquemas=None, columna_incremental=\"fecalta\"):\n",
    "    \"\"\"\n",
    "    Lee múltiples tablas desde Azure SQL Database, usando carga full o incremental por tabla.\n",
    "\n",
    "    Parámetros:\n",
    "    - lista_tablas: lista de nombres de tablas a leer.\n",
    "    - lista_modos_carga: lista del mismo tamaño que lista_tablas con 'full' o 'incremental'.\n",
    "    - fecha_corte: fecha para filtro incremental.\n",
    "    - url_conexion: cadena JDBC a Azure SQL.\n",
    "    - usuario: nombre de usuario de conexión.\n",
    "    - password: contraseña.\n",
    "    - esquemas: dict opcional {tabla: esquema}, por defecto usa 'dbo'.\n",
    "    - columna_incremental: nombre de la columna de fecha para carga incremental.\n",
    "\n",
    "    Retorna:\n",
    "    - None. Crea variables globales con los DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(lista_tablas) != len(lista_modos_carga):\n",
    "        raise ValueError(\"Las listas 'lista_tablas' y 'lista_modos_carga' deben tener la misma longitud.\")\n",
    "\n",
    "    properties = {\n",
    "        \"user\": usuario,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "\n",
    "    for i, nombre_tabla in enumerate(lista_tablas):\n",
    "        modo_carga = lista_modos_carga[i].lower()\n",
    "        esquema = esquemas.get(nombre_tabla, \"dbo\") if esquemas else \"dbo\"\n",
    "        tabla_completa = f\"{esquema}.{nombre_tabla}\"\n",
    "        query = \"\"\n",
    "        base_query = f\"(SELECT * FROM {tabla_completa}) AS temp\"\n",
    "\n",
    "        try:\n",
    "            # Validar existencia de columna incremental\n",
    "            df_sample = spark.read.jdbc(url=url_conexion, table=base_query, properties=properties).limit(10)\n",
    "\n",
    "            if modo_carga == \"incremental\":\n",
    "                if columna_incremental in df_sample.columns:\n",
    "                    query = f\"(SELECT * FROM {tabla_completa} WHERE {columna_incremental} = '{fecha_corte}') AS temp\"\n",
    "                else:\n",
    "                    print(f\"⚠️  Tabla '{tabla_completa}' sin columna '{columna_incremental}'. Se leerá completa.\")\n",
    "                    query = base_query\n",
    "            else:\n",
    "                query = base_query\n",
    "\n",
    "            df = spark.read.jdbc(url=url_conexion, table=query, properties=properties)\n",
    "            globals()[nombre_tabla] = df\n",
    "\n",
    "            print(f\"✅ DataFrame '{nombre_tabla}' ({modo_carga}) cargado desde '{tabla_completa}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al leer la tabla '{tabla_completa}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d7a071-3f0c-438d-b922-6a9f17ea4183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parámetros de conexión\n",
    "url_conexion = dbutils.secrets.get(\"scope-dev\", \"secret-sql-url\")\n",
    "usuario = dbutils.secrets.get(\"scope-dev\", \"secret-sql-user\")\n",
    "password = dbutils.secrets.get(\"scope-dev\", \"secret-sql-password\")\n",
    "fecha_carga = \"2025-04-14\"\n",
    "#properties = {\"user\": \"admin01juls\", \"password\": \"287719Julius@12\", \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "\n",
    "# Lista de tablas a leer\n",
    "lista_tablas = [\"Camas\", \"Habitaciones\", \"Medicos\", \"Pacientes\",\"Internamientos\",\"Visitas_Medicas\"]\n",
    "esquemas = {\"Camas\": \"st_thomas\", \"Visitas_Medicas\": \"St_thomas\", \"Habitaciones\": \"st_thomas\", \"Medicos\": \"st_thomas\", \"Pacientes\": \"st_thomas\", \"Internamientos\": \"st_thomas\"}  # pacientes usa 'dbo' por defecto\n",
    "modos = [\"incremental\", \"full\", \"incremental\", \"incremental\", \"full\", \"full\"]\n",
    "\n",
    "# Llamar la función para crear DataFrames individuales\n",
    "leer_tablas_sql_azure(lista_tablas, modos, fecha_carga, url_conexion, usuario, password, esquemas, columna_incremental=\"fecalta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477c9e26-cf54-4760-b625-3a4e3719bc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import col, lit , to_timestamp, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "\n",
    "df_Camas= Camas.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Camas.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Camas.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8daed60f-7f5c-4407-a1bc-b5a3b6e257ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Habitaciones = Habitaciones.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Habitaciones.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Habitaciones.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85763ad2-6ea1-411c-aad2-46a1ccfc5be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Pacientes = Pacientes.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Pacientes.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Pacientes.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2fde24-1ff0-40eb-8ba7-f2ec3a7db257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Medicos = Medicos.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Medicos.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Medicos.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6931f02f-52df-45b0-beb9-3365e45f3409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_Internamientos = Internamientos.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_Internamientos.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_Internamientos.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb22a931-3ced-4287-85db-aeaecd50fe51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_VisitasMedicas = Visitas_Medicas.withColumn(\"year\",  col(\"fecalta\").substr(1, 4)) \\\n",
    "    .withColumn(\"month\", col(\"fecalta\").substr(6, 2)) \\\n",
    "    .withColumn(\"day\", col(\"fecalta\").substr(9, 2)) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"fecalta\", to_timestamp(\"fecalta\"))\n",
    "\n",
    "df_VisitasMedicas.printSchema()\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para ver la relación\n",
    "df_VisitasMedicas.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c20368-3c36-4f7b-bfaa-c7babef14f3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1464b6-cad3-42fe-bc79-967651597945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def crear_tabla_delta_merge_managed(\n",
    "    nombre_df: str,\n",
    "    nombre_tabla: str,\n",
    "    llave_origen: List[str],\n",
    "    llave_destino: List[str],\n",
    "    db_name: str = \"default\",\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    auto_merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea si no existe una tabla Delta GESTIONADA en la base (que ya debe tener LOCATION en tu mount)\n",
    "    y realiza MERGE. No usa LOCATION explícito.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validaciones\n",
    "    df = globals()[nombre_df]\n",
    "\n",
    "    if len(llave_origen) != len(llave_destino):\n",
    "        print(\"❌ Error: La cantidad de columnas en 'llave_origen' y 'llave_destino' no coinciden.\")\n",
    "        return\n",
    "\n",
    "    if partition_cols:\n",
    "        faltantes = [c for c in partition_cols if c not in df.columns]\n",
    "        if faltantes:\n",
    "            print(f\"❌ Error: Columnas de partición no existen en el DataFrame: {faltantes}\")\n",
    "            return\n",
    "\n",
    "    if auto_merge_schema:\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Armar nombre completo\n",
    "    full_name = f\"{catalog_name}.{db_name}.{nombre_tabla}\"\n",
    "\n",
    "    # ✅ FIX: usar el overload moderno (una sola cadena)\n",
    "    exists = spark.catalog.tableExists(full_name)\n",
    "\n",
    "    if not exists:\n",
    "        # Crear como TABLA GESTIONADA en el LOCATION de la DB (sin LOCATION explícito)\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_cols:\n",
    "            # ✅ FIX: varargs\n",
    "            writer = writer.partitionBy(*partition_cols)\n",
    "        writer.saveAsTable(full_name)\n",
    "        print(f\"✅ Tabla gestionada creada: {full_name} (bajo LOCATION de la base '{db_name}')\")\n",
    "        return\n",
    "\n",
    "    # Si existe, MERGE\n",
    "    try:\n",
    "        delta_tbl = DeltaTable.forName(spark, full_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"❌ La tabla {full_name} no es Delta o no es accesible como Delta: {e}\")\n",
    "\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"tgt.`{llave_destino[i]}` = src.`{llave_origen[i]}`\" for i in range(len(llave_origen))]\n",
    "    )\n",
    "    set_expr  = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "    vals_expr = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "\n",
    "    print(f\"🔄 Ejecutando MERGE INTO {full_name} ...\")\n",
    "    (delta_tbl.alias(\"tgt\")\n",
    "             .merge(df.alias(\"src\"), merge_condition)\n",
    "             .whenMatchedUpdate(set=set_expr)\n",
    "             .whenNotMatchedInsert(values=vals_expr)\n",
    "             .execute())\n",
    "    print(f\"✅ MERGE completado para {full_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895b119a-35b6-4a24-b483-a6e53f20049e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar la función para crear la tabla y hacer MERGE usando diferentes llaves\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Camas\",\n",
    "    nombre_tabla=\"t_camas\",\n",
    "    llave_origen=[\"numero_cama\"],\n",
    "    llave_destino=[\"numero_cama\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Habitaciones\",\n",
    "    nombre_tabla=\"t_habitaciones\",\n",
    "    llave_origen=[\"numero_habitacion\"],\n",
    "    llave_destino=[\"numero_habitacion\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Pacientes\",\n",
    "    nombre_tabla=\"t_pacientes\",\n",
    "    llave_origen=[\"id_paciente\"],\n",
    "    llave_destino=[\"id_paciente\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Medicos\",\n",
    "    nombre_tabla=\"t_medicos\",\n",
    "    llave_origen=[\"id_medico\"],\n",
    "    llave_destino=[\"id_medico\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Internamientos\",\n",
    "    nombre_tabla=\"t_internamientos\",\n",
    "    llave_origen=[\"id_internamiento\"],\n",
    "    llave_destino=[\"id_internamiento\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_VisitasMedicas\",\n",
    "    nombre_tabla=\"t_visitas_medicas\",\n",
    "    llave_origen=[\"id_visita\"],\n",
    "    llave_destino=[\"id_visita\"],\n",
    "    db_name=\"bronze_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partición, quítalo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c70a35-f0c8-4d6a-b88f-bc3e0e3ded45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def tune_delta_table(db_name: str, table_name: str, catalog_name: str = \"desarrollo\", zorder_cols=None, retention_hours: int = 168) -> None:\n",
    "    \"\"\"\n",
    "    Ajusta una tabla Delta con OPTIMIZE (+ZORDER opcional) y VACUUM.\n",
    "    - retention_hours >= 168 (7 días) para cumplir la regla por defecto.\n",
    "    \"\"\"\n",
    "    full = f\"{catalog_name}.{db_name}.{table_name}\"\n",
    "\n",
    "    # Buenas prácticas por sesión (puedes mover a init script/cluster)\n",
    "    spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "\n",
    "    # Asegura propiedades en la tabla\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {full} SET TBLPROPERTIES (\n",
    "          delta.autoOptimize.optimizeWrite = true,\n",
    "          delta.autoOptimize.autoCompact = true\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    # OPTIMIZE (+ ZORDER opcional)\n",
    "    if zorder_cols:\n",
    "        cols = \", \".join([f\"`{c}`\" for c in zorder_cols])\n",
    "        spark.sql(f\"OPTIMIZE {full} ZORDER BY ({cols})\")\n",
    "    else:\n",
    "        spark.sql(f\"OPTIMIZE {full}\")\n",
    "\n",
    "    # VACUUM con retención segura (>= 7 días)\n",
    "    spark.sql(f\"VACUUM {full} RETAIN {retention_hours} HOURS\")\n",
    "    print(f\"✅ OPTIMIZE/VACUUM completados para {full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f0ec8f-d6cf-49bb-982d-209a1c6869e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tune_delta_table(\"bronze_clinica\", \"t_camas\", zorder_cols=[\"numero_cama\"])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3555005416841897,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc_clinica01_extraccion_bronze_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
