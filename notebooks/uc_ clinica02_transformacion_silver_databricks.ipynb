{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236f459c-d109-42fc-ae40-556906754c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Transformaci√≥n de Datos: Capa Silver en Azure Databricks\n",
    "\n",
    "Este notebook lee datos desde la capa Bronze (Delta Lake gestionada) y aplica transformaciones y limpieza antes de escribir en la **capa Silver** utilizando `MERGE INTO` para control de cambios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c914f1e1-fe47-4d12-bd22-77ab6f299abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f4bb6b-25ee-4ec0-80fd-22c61abbeaf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def leer_desde_bronze(nombre_tabla: str, catalog_name: str = \"desarrollo\", db_bronze: str = \"bronze_clinica\"):\n",
    "    full = f\"{catalog_name}.{db_bronze}.{nombre_tabla}\"\n",
    "    if not spark.catalog.tableExists(full):\n",
    "        raise ValueError(f\"La tabla {full} no existe en el metastore.\")\n",
    "    return spark.table(full)  # o spark.read.table(full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5176d0-6dde-4136-a29c-e5fdeb28dc10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "# Leer desde bronze\n",
    "df_Camas = leer_desde_bronze(\"t_camas\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_Camas = df_Camas.fillna({\n",
    "    \"numero_cama\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"numero_habitacion\": 0,     # Reemplaza nulos en numero_habitacion  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_Camas = df_Camas.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Camas.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Camas.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ed6980-443d-4938-be15-533594cc8c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_Habitaciones = leer_desde_bronze(\"t_habitaciones\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_Habitaciones = df_Habitaciones.fillna({\n",
    "    \"numero_habitacion\": 0, \"piso\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"piso\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_Habitaciones = df_Habitaciones.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Habitaciones.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Habitaciones.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449ce181-9ced-44f8-875e-e3e65bf80c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_Pacientes = leer_desde_bronze(\"t_pacientes\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_Pacientes = df_Pacientes.fillna({\n",
    "    \"id_paciente\": 0,  \"genero\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_Pacientes = df_Pacientes.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Pacientes.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Pacientes.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded5e93e-f58f-4a39-83e3-df2ca3a637f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_Medicos = leer_desde_bronze(\"t_medicos\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_Medicos = df_Medicos.fillna({\n",
    "    \"id_medico\": 0, \"estado\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_Medicos = df_Medicos.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Medicos.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Medicos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fb8f8f-e625-4009-ade8-3156df3fe132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_Internamientos = leer_desde_bronze(\"t_internamientos\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_Internamientos = df_Internamientos.fillna({\n",
    "    \"id_internamiento\": 0, \"id_paciente\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"id_medico\": 0, \"numero_cama\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_Internamientos = df_Internamientos.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_Internamientos.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_Internamientos.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945df719-82ed-4a73-b411-03997fcdbd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer desde bronze\n",
    "df_VisitasMedicas = leer_desde_bronze(\"t_visitas_medicas\", db_bronze=\"bronze_clinica\")\n",
    "\n",
    "# Paso 1: Manejo de valores nulos\n",
    "df_VisitasMedicas = df_VisitasMedicas.fillna({\n",
    "    \"id_visita\": 0, \"id_internamiento\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"id_medico\": 0,  # Reemplaza nulos en numero_cama  con 0\n",
    "    \"fecalta\": \"1970-01-01 00:00:00\"  # Reemplaza fechas nulas con una fecha por defecto\n",
    "})\n",
    "\n",
    "# Paso 2: Eliminaci√≥n de duplicados (basado en todas las columnas)\n",
    "df_VisitasMedicas = df_VisitasMedicas.dropDuplicates()\n",
    "\n",
    "# Mostrar la estructura del DataFrame despu√©s de la limpieza\n",
    "df_VisitasMedicas.printSchema()\n",
    "\n",
    "# Contar filas antes y despu√©s de la limpieza para validar cambios\n",
    "print(f\"Total de filas despu√©s de la limpieza: {df_VisitasMedicas.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24f0ce1-11cc-4b27-b725-c5b7708fa2ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG desarrollo;\n",
    "\n",
    "SHOW DATABASES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f239cb22-5a13-4eaa-a7c4-902432956a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def crear_tabla_delta_merge_managed(\n",
    "    nombre_df: str,\n",
    "    nombre_tabla: str,\n",
    "    llave_origen: List[str],\n",
    "    llave_destino: List[str],\n",
    "    db_name: str = \"default\",\n",
    "    catalog_name: str = \"desarrollo\",\n",
    "    partition_cols: Optional[List[str]] = None,\n",
    "    auto_merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea si no existe una tabla Delta GESTIONADA en la base (que ya debe tener LOCATION en tu mount)\n",
    "    y realiza MERGE. No usa LOCATION expl√≠cito.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validaciones\n",
    "    df = globals()[nombre_df]\n",
    "\n",
    "    if len(llave_origen) != len(llave_destino):\n",
    "        print(\"‚ùå Error: La cantidad de columnas en 'llave_origen' y 'llave_destino' no coinciden.\")\n",
    "        return\n",
    "\n",
    "    if partition_cols:\n",
    "        faltantes = [c for c in partition_cols if c not in df.columns]\n",
    "        if faltantes:\n",
    "            print(f\"‚ùå Error: Columnas de partici√≥n no existen en el DataFrame: {faltantes}\")\n",
    "            return\n",
    "\n",
    "    if auto_merge_schema:\n",
    "        spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Armar nombre completo\n",
    "    full_name = f\"{catalog_name}.{db_name}.{nombre_tabla}\"\n",
    "\n",
    "    # ‚úÖ FIX: usar el overload moderno (una sola cadena)\n",
    "    exists = spark.catalog.tableExists(full_name)\n",
    "\n",
    "    if not exists:\n",
    "        # Crear como TABLA GESTIONADA en el LOCATION de la DB (sin LOCATION expl√≠cito)\n",
    "        writer = df.write.format(\"delta\").mode(\"overwrite\")\n",
    "        if partition_cols:\n",
    "            # ‚úÖ FIX: varargs\n",
    "            writer = writer.partitionBy(*partition_cols)\n",
    "        writer.saveAsTable(full_name)\n",
    "        print(f\"‚úÖ Tabla gestionada creada: {full_name} (bajo LOCATION de la base '{db_name}')\")\n",
    "        return\n",
    "\n",
    "    # Si existe, MERGE\n",
    "    try:\n",
    "        delta_tbl = DeltaTable.forName(spark, full_name)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"‚ùå La tabla {full_name} no es Delta o no es accesible como Delta: {e}\")\n",
    "\n",
    "    merge_condition = \" AND \".join(\n",
    "        [f\"tgt.`{llave_destino[i]}` = src.`{llave_origen[i]}`\" for i in range(len(llave_origen))]\n",
    "    )\n",
    "    set_expr  = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "    vals_expr = {c: f\"src.`{c}`\" for c in df.columns}\n",
    "\n",
    "    print(f\"üîÑ Ejecutando MERGE INTO {full_name} ...\")\n",
    "    (delta_tbl.alias(\"tgt\")\n",
    "             .merge(df.alias(\"src\"), merge_condition)\n",
    "             .whenMatchedUpdate(set=set_expr)\n",
    "             .whenNotMatchedInsert(values=vals_expr)\n",
    "             .execute())\n",
    "    print(f\"‚úÖ MERGE completado para {full_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f585d9d-efac-43c7-9423-781873edaba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar la funci√≥n para crear la tabla y hacer MERGE usando diferentes llaves\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Camas\",\n",
    "    nombre_tabla=\"md_camas\",\n",
    "    llave_origen=[\"numero_cama\"],\n",
    "    llave_destino=[\"numero_cama\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Habitaciones\",\n",
    "    nombre_tabla=\"md_habitaciones\",\n",
    "    llave_origen=[\"numero_habitacion\"],\n",
    "    llave_destino=[\"numero_habitacion\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Pacientes\",\n",
    "    nombre_tabla=\"md_pacientes\",\n",
    "    llave_origen=[\"id_paciente\"],\n",
    "    llave_destino=[\"id_paciente\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Medicos\",\n",
    "    nombre_tabla=\"md_medicos\",\n",
    "    llave_origen=[\"id_medico\"],\n",
    "    llave_destino=[\"id_medico\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_Internamientos\",\n",
    "    nombre_tabla=\"md_internamientos\",\n",
    "    llave_origen=[\"id_internamiento\"],\n",
    "    llave_destino=[\"id_internamiento\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n",
    "\n",
    "crear_tabla_delta_merge_managed(\n",
    "    nombre_df=\"df_VisitasMedicas\",\n",
    "    nombre_tabla=\"md_visitas_medicas\",\n",
    "    llave_origen=[\"id_visita\"],\n",
    "    llave_destino=[\"id_visita\"],\n",
    "    db_name=\"silver_clinica\",\n",
    "    partition_cols=[\"fecalta\"]  # opcional; si no quieres partici√≥n, qu√≠talo\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3522282755143131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "uc_ clinica02_transformacion_silver_databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
