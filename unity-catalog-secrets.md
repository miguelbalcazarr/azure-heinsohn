# Uso de Secret Scope `scope-dev` y secrets en Azure Databricks

## üìå Descripci√≥n general
Este documento describe c√≥mo los notebooks consumen credenciales de **Azure Key Vault** a trav√©s del **Secret Scope** `scope-dev` en Azure Databricks, evitando exponer credenciales en c√≥digo y permitiendo la rotaci√≥n centralizada v√≠a RBAC.

---

## ‚úÖ Requisitos previos (alto nivel)
- **Azure Key Vault** con los secretos creados.
- **Azure RBAC** habilitado en el Key Vault.
- La **Managed Identity** (UAMI/SAMI) del workspace con rol **Key Vault Secrets User** sobre el Key Vault.
- **Secret Scope** `scope-dev` en Databricks apuntando a ese Key Vault.

> Nota: Los nombres de las *properties* y *actions* van en **English** (ej. `scope`, `key`, `get`), las explicaciones en espa√±ol.

---

## üîë Scope y secrets utilizados

**Scope:** `scope-dev`

**Secrets (keys):**
- `secret-sql-ventas-url` ‚Üí URL JDBC de la BD Ventas  
- `secret-sql-ventas-user` ‚Üí Usuario de la BD Ventas  
- `secret-sql-ventas-password` ‚Üí Password del usuario de la BD Ventas  
- `secret-sql-clinica-url` ‚Üí URL JDBC de la BD Clinica  
- `secret-sql-clinica-user` ‚Üí Usuario de la BD Clinica  
- `secret-sql-clinica-password` ‚Üí Password del usuario de la BD Clinica  

---

## üß™ Verificaci√≥n r√°pida

Listar scopes:
```python
dbutils.secrets.listScopes()
```

Listar keys dentro del scope:
```python
dbutils.secrets.list("scope-dev")
```

> Los valores de los secrets **no** se imprimen por seguridad; solo ver√°s los nombres de las keys.

---

## ‚öôÔ∏è Consumo en notebooks

### 1) Read secrets
```python
url_conexion = dbutils.secrets.get("scope-dev", "secret-sql-ventas-url")
usuario      = dbutils.secrets.get("scope-dev", "secret-sql-ventas-user")
password     = dbutils.secrets.get("scope-dev", "secret-sql-ventas-password")
```

```python
url_conexion = dbutils.secrets.get("scope-dev", "secret-sql-clinica-url")
usuario      = dbutils.secrets.get("scope-dev", "secret-sql-clinica-user")
password     = dbutils.secrets.get("scope-dev", "secret-sql-clinica-password")
```

### 2) Use en conexi√≥n JDBC (Spark)
```python
df = (spark.read.format("jdbc")
      .option("url", url_conexion)
      .option("user", usuario)
      .option("password", password)
      .option("dbtable", "dbo.FactVentas")
      .load())
display(df)
```

### 3) Write (opcional)
```python
(df.write.mode("append")
   .format("jdbc")
   .option("url", url_conexion)
   .option("user", usuario)
   .option("password", password)
   .option("dbtable", "dbo.LogIngesta")
   .save())
```

---

## üîí Buenas pr√°cticas
- **No** imprimas ni loguees variables que contengan secretos.
- Usa **grupos de AAD** (no usuarios individuales) para asignar acceso en Key Vault.
- Separa scopes por entorno: `scope-dev`, `scope-qa`, `scope-prd`.
- Versiona en Git solo el **nombre** del scope/keys; jam√°s exportes valores de secretos.

---

## üõ†Ô∏è Troubleshooting
- **`PERMISSION_DENIED: Failed to read secret from scope`**  
  - Verifica que el scope existe: `dbutils.secrets.listScopes()`  
  - Asegura que la Managed Identity tenga **Key Vault Secrets User** en el KV.  
  - Chequea que el scope est√© apuntando al **Key Vault correcto** (DNS/Resource ID).

- **`KeyError: Secret not found`**  
  - Asegura que la key existe en KV y coincide exactamente:  
    `secret-sql-ventas-url`, `secret-sql-ventas-user`, `secret-sql-ventas-password`.

- **Rotaci√≥n de credenciales**  
  - Rota en **Key Vault** y mant√©n el mismo nombre de key; los notebooks no requieren cambios.

---

## üìö Referencias
- Azure Databricks ‚Äì Secret Scopes  
- Azure Databricks ‚Äì Azure Key Vault‚Äìbacked scopes  
- Azure Key Vault ‚Äì RBAC Guide
